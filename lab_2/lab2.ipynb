{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, List\n",
    "\n",
    "sw = open('stop_words.txt','r',encoding='utf-8')\n",
    "sw = sw.read().split('\\n')\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "def read_texts(fn: str=\"news.txt.gz\") -> Iterator[Text]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield Text(*line.strip().split(\"\\t\"))\n",
    "                                  \n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    words = [w for w in re.findall(r'\\b\\w+\\b', text.lower()) if w not in sw]\n",
    "    return words\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    return ' '.join(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация с помощью word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [tokenize_text(text.text) for text in read_texts()]\n",
    "\n",
    "w2v = Word2Vec(sentences,vector_size=250)\n",
    "\n",
    "w2v.wv.save_word2vec_format('w2v_vectors.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация текста путем усреднения векторов слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_emb =[]\n",
    "labels  = []\n",
    "lens = []\n",
    "for text in read_texts():\n",
    "    labels.append(text.label)\n",
    "    words_vecs = [w2v.wv[word] for word in tokenize_text(text.text) if word in w2v.wv]\n",
    "    lens.append(len(words_vecs))\n",
    "    mean_emb.append(np.mean(words_vecs,axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'label':labels})\n",
    "classes = list(set(labels))\n",
    "df['label']=df['label'].apply(classes.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "X_train,X_test,y_train,y_test = train_test_split(np.array(mean_emb),df['label'],test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.687"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление к векторам слов эмбеддинга длины текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "len_emb = torch.nn.Embedding(np.max(lens)+1,15,max_norm=True)\n",
    "lens_emb = len_emb(torch.tensor(lens))\n",
    "new_emb = np.concatenate((np.array(mean_emb),lens_emb.detach().numpy()),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6803333333333333"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "X_train,X_test,y_train,y_test = train_test_split(np.array(new_emb),df['label'],test_size=0.3,random_state=42)\n",
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Взвешивание векторов слов с помощью tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "texts = list(read_texts(\"news.txt.gz\"))\n",
    "vectorizer = TfidfVectorizer(min_df=3,stop_words=sw)\n",
    "tfidf = vectorizer.fit([normalize_text(text.text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = dict(zip(tfidf.get_feature_names_out(),list(tfidf.idf_)))\n",
    "names =tfidf.get_feature_names_out()\n",
    "words = list(w2v.wv.key_to_index.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sklearn токенизирует по своему, поэтому в словаре tf-idf отсутствуют некоторые токены. Вместо проверки на наличее слова из текста в двух словарях, один из которых может быть значительно больше другого, находится перекрытие между словарями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_uni =[]\n",
    "for i in words:\n",
    "    if i not in names:\n",
    "        wv_uni.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tfidf_w2v = []\n",
    "for text in read_texts():\n",
    "    labels.append(text.label)\n",
    "    tok_text = tokenize_text(text.text)\n",
    "    words_vecs = [w2v.wv[word] for word in tok_text if word in w2v.wv and word not in wv_uni]\n",
    "    idfs_w = [idfs[word] for word in tok_text if word in w2v.wv and word not in wv_uni]\n",
    "    tfs = Counter(tok_text)\n",
    "    tff = [tfs[word]/len(words_vecs) for word in tok_text if word in w2v.wv and word not in wv_uni]\n",
    "    tf_id = (tff*np.array(idfs_w))\n",
    "    norm = sum(tff*np.array(idfs_w))\n",
    "    tfidf_w2v.append(np.sum((np.array(words_vecs).T*tf_id).T,axis=0)/norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 250)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tfidf_w2v).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "X_train,X_test,y_train,y_test = train_test_split(np.array(tfidf_w2v),df['label'],test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.692"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
